 so then tim peter email is all like  [tim]   my test train on about number msgs and a binari pickl of the databas is   approach number million byte   that shrink to under number million byte though if i delet all the wordinfo  record with spamprob exact equal to unknown_spamprob such record  aren't need when score (an unknown word get a made-up probabl of  unknown_spamprob) such record are onli need for train i'v note  befor that a scoring-on databas can be leaner that pretti good i wonder how much better you could do by use some custom pickler i just check my littl dbm file and found a lot of what i would call bloat  import anydbm hammi  d = hammie.persistentgrahambayes("ham.db")  db = anydbm.open("ham.db")  db["neale"] len(db["neale"]) number number  d.wordinfo["neale"] len(`d.wordinfo["neale"]`) number number number number number number ignor the fact that there are too mani zero in there the pickl version of that wordinfo object is over twice as larg as the string represent so we could get a percent decreas in size just by use the string represent instead of the pickl right someth about that logic seem wrong to me but i can't see what it is mayb pickl is good for heterogen data type but everi valu of our big dictionari is go to have the same type so there a ton of redund i guess that explain whi it compress so well neal